@misc{wu2022sustainable,
	title={Sustainable AI: Environmental Implications, Challenges and Opportunities}, 
	author={Carole-Jean Wu and Ramya Raghavendra and Udit Gupta and Bilge Acun and Newsha Ardalani and Kiwan Maeng and Gloria Chang and Fiona Aga Behram and James Huang and Charles Bai and Michael Gschwind and Anurag Gupta and Myle Ott and Anastasia Melnikov and Salvatore Candido and David Brooks and Geeta Chauhan and Benjamin Lee and Hsien-Hsin S. Lee and Bugra Akyildiz and Maximilian Balandat and Joe Spisak and Ravi Jain and Mike Rabbat and Kim Hazelwood},
	year={2022},
	eprint={2111.00364},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}


@INPROCEEDINGS{albiol2009,
	author={Albiol, Antonio and Silla, Julia and Albiol, Alberto and Mossi, J.M. and Sanchis, Laura},
	booktitle={3rd International Conference on Imaging for Crime Detection and Prevention (ICDP 2009)}, 
	title={Automatic video annotation and event detection for video surveillance}, 
	year={2009},
	volume={},
	number={},
	pages={1-5},
	doi={10.1049/ic.2009.0270}}

@INPROCEEDINGS{chakraborty2021,  author={Chakraborty, Dipanita and Chiracharit, Werapon and Chamnongthai, Kosin},  booktitle={2021 18th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)},   title={Video Shot Boundary Detection Using Principal Component Analysis (PCA) and Deep Learning},   year={2021},  volume={},  number={},  pages={272-275},  doi={10.1109/ECTI-CON51831.2021.9454775}}

@INPROCEEDINGS{swaminathan2019,
	author={Swaminathan, Vaibhav and Arora, Shrey and Bansal, Ravi and Rajalakshmi, R},
	booktitle={2019 International Conference on Computational Intelligence in Data Science (ICCIDS)}, 
	title={Autonomous Driving System with Road Sign Recognition using Convolutional Neural Networks}, 
	year={2019},
	volume={},
	number={},
	pages={1-4},
	doi={10.1109/ICCIDS.2019.8862152}}

@INPROCEEDINGS{limmer2017,  author={Limmer, Matthias and Forster, Julian and Baudach, Dennis and Schüle, Florian and Schweiger, Roland and Lensch, Hendrik P.A.},  booktitle={2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)},   title={Robust Deep-Learning-Based Road-Prediction for Augmented Reality Navigation Systems at Night},   year={2016},  volume={},  number={},  pages={1888-1895},  doi={10.1109/ITSC.2016.7795862}}

@INPROCEEDINGS{aung2021,
	author={Aung, Htet and Bobkov, Alexander V. and Tun, Nyan Lin},
	booktitle={2021 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)}, 
	title={Face Detection in Real Time Live Video Using Yolo Algorithm Based on Vgg16 Convolutional Neural Network}, 
	year={2021},
	volume={},
	number={},
	pages={697-702},
	doi={10.1109/ICIEAM51226.2021.9446291}}



@article{khan2020,
title = {A survey of the recent architectures of deep convolutional neural networks},
journal = {Artificial Intelligence Review},
volume = {53},
pages = {5455–5516},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1007/s10462-020-09825-6},
author = {Asifullah Khan and Anabia Sohail and Umme Zahoora and Aqsa Saeed Qureshi },
}

@article{algan2021,
title = {Image classification with deep learning in the presence of noisy labels: A survey},
journal = {Knowledge-Based Systems},
volume = {215},
pages = {106771},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106771},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121000344},
author = {Görkem Algan and Ilkay Ulusoy},
keywords = {Deep learning, Label noise, Classification with noise, Noise robust, Noise tolerant},
abstract = {Image classification systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or difficulty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in real-world datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overfit data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels. Differently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.}
}

@misc{wu2021,
      title={A Bayesian Approach to (Online) Transfer Learning: Theory and Algorithms},
      author={Xuetong Wu and Jonathan H. Manton and Uwe Aickelin and Jingge Zhu},
      year={2021},
      eprint={2109.01377},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zhu2018,
title = {Transfer learning with stacked reconstruction independent component analysis},
journal = {Knowledge-Based Systems},
volume = {152},
pages = {100-106},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118301758},
author = {Yi Zhu and Xuegang Hu and Yuhong Zhang and Peipei Li},
keywords = {Stacked RICA, Transfer learning, Logistic regression model, KL-Divergence},
abstract = {Significant improvements to transfer learning have emerged in recent years, because deep learning has been proposed to learn more higher level and robust features. However, most of existing deep learning approaches are based on the framework of auto-encoder or sparse auto-encoder, which pose challenges for independent component analysis and fail to measure similarities between data spaces. Therefore, in this paper, we propose a new strategy to achieve a better feature representation performance for transfer learning. There are several advantages in our method as follows: 1) The model of Stacked Reconstruction Independent Component Analysis (SRICA) is used to pursuit an optimal feature representation; 2) The label information is used by Logistic Regression Model to optimize representation features and the distance of distributions between domains is minimized by the method of KL-Divergence. Extensive experiments conducted on several image datasets demonstrate the superiority of our proposed method compared with all competing state-of-the-art methods.}
}

@article{tan2021,
title = {Improving knowledge distillation via an expressive teacher},
journal = {Knowledge-Based Systems},
volume = {218},
pages = {106837},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106837},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001003},
author = {Chao Tan and Jie Liu and Xiang Zhang},
keywords = {Neural network compression, Knowledge distillation, Knowledge transfer},
}

@article{zhao2021,
title = {Knowledge distillation via instance-level sequence learning},
journal = {Knowledge-Based Systems},
volume = {233},
pages = {107519},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107519},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007814},
author = {Haoran Zhao and Xin Sun and Junyu Dong and Zihe Dong and Qiong Li},
keywords = {Neural networks compression, Knowledge distillation, Computer vision, Deep learning},
}

@misc{lee2021,
      title={Visualizing the embedding space to explain the effect of knowledge distillation},
      author={Hyun Seung Lee and Christian Wallraven},
      year={2021},
      eprint={2110.04483},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{pzhao2021,
title = {Robust transfer learning based on Geometric Mean Metric Learning},
journal = {Knowledge-Based Systems},
volume = {227},
pages = {107227},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107227},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121004895},
author = {Peng Zhao and Tao Wu and Shiyi Zhao and Huiting Liu},
keywords = {Transfer learning, Geometric Mean Metric Learning, Domain adaptation},
abstract = {Transfer learning usually utilizes the knowledge learned from the relative labeled source domain to promote the model performance in the unlabeled or few labeled target domain with different distribution. Most of the existing transfer learning methods aim to reduce the discrepancy of distributions between the source and target domains, but ignore the discriminative category information involved in the data from both domains in the process of knowledge transfer. To learn more discriminative feature representation in knowledge transfer, this paper integrates the transfer learning and metric learning into a unified framework and proposes a novel robust transfer learning based on geometric mean metric learning, namely Geometric Mean Transfer Learning (GMTL). GMTL uses weighted geometric mean metric learning to model the intra-class distance and the inter-class similarity. In the meantime, the marginal distributions and conditional distributions of the source and target domains are jointly adapted. Moreover, according to the natures of the datasets in different tasks, we dynamically combine the discriminative modeling and domain adaption to make the proposed model more robust. We assign different weights to the intra-class distance and the inter-class similarity in metric learning and different weights to marginal distribution adaption and conditional distribution adaption, respectively. Finally, the solution to the objective function is converted to the problem of finding a point on the geodesic joining two points on the Riemannian manifold, which is very simple and direct. Extensive experiments are conducted on six datasets widely adopted in transfer learning to verify the superiority of our proposed GMTL over existing state-of-the-art transfer learning methods.}
}

@inproceedings{krizhevsky2012,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {1097--1105},
	publisher = {Curran Associates, Inc.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	doi = {10.1145/3065386},
	volume = {25},
	year = {2012}
}

@ARTICLE{huang2021,
	author={F. {Zhuang} and Z. {Qi} and K. {Duan} and D. {Xi} and Y. {Zhu} and H. {Zhu} and H. {Xiong} and Q. {He}},
	journal={Proceedings of the IEEE},
	title={A Comprehensive Survey on Transfer Learning},
	year={2021},
	volume={109},
	number={1},
	pages={43-76},
	doi={10.1109/JPROC.2020.3004555}}


@article{liu2020,
  author    = {Linqing Liu and
               Huan Wang and
               Jimmy Lin and
               Richard Socher and
               Caiming Xiong},
  title     = {MKD: a Multi-Task Knowledge Distillation Approach for Pretrained Language Models},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1911.03588},
  timestamp = {Sun, 01 Dec 2019 20:31:34 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hinton2015,
	title	= {Distilling the Knowledge in a Neural Network},
	author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
	year	= {2015},
	booktitle	= {NIPS Deep Learning and Representation Learning Workshop}
}


@misc{gou2020,
	title={Knowledge Distillation: A Survey},
	author={Jianping Gou and Baosheng Yu and Stephen John Maybank and Dacheng Tao},
	year={2020},
	eprint={2006.05525},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{geyer2019,
	title={Transfer Learning by Adaptive Merging of Multiple Models},
	author={Geyer, Robin C. and Wegmayr, Viktor and Corinzia, Luca},
	booktitle={International Conference on Medical Imaging with Deep Learning -- Full Paper Track},
	address={London, United Kingdom},
	year={2019},
	month={08--10 Jul},
}

@book{goodfellow2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	year={2016},
	doi={10.1007/s10710-017-9314-z}
}

@inproceedings{lee2017,
	author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
	title = {Overcoming Catastrophic Forgetting by Incremental Moment Matching},
	year = {2017},
	isbn = {9781510860964},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	pages = {4655–4665},
	numpages = {11},
	location = {Long Beach, California, USA},
	series = {NIPS'17}
}

@inproceedings{wasay2020,
	author = {Wasay, Abdul and Hentschel, Brian and Liao, Yuze and Chen, Sanyuan and Idreos, Stratos},
	booktitle = {Proceedings of Machine Learning and Systems},
	editor = {I. Dhillon and D. Papailiopoulos and V. Sze},
	pages = {199--215},
	title = {MotherNets: Rapid Deep Ensemble Learning},
	volume = {2},
	year = {2020}
}

@inproceedings{asif2019,
	author = {{Asif}, Umar and {Tang}, Jianbin and {Harrer}, Stefan},
	title = "{Ensemble Knowledge Distillation for Learning Improved and Efficient Networks}",
	booktitle = {24th European Conference on Artificial Intelligence - ECAI},
	year = 2020,
}

@misc{chollet2015keras,
	title={Keras},
	author={Chollet, Fran\c{c}ois and others},
	year={2015},
	howpublished={\url{https://keras.io}},
}

@INPROCEEDINGS{he2016,
	author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	title={Deep Residual Learning for Image Recognition},
	year={2016},
	volume={},
	number={},
	pages={770-778},
	doi={10.1109/CVPR.2016.90}}

@inproceedings{szegedy2017,
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
	title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
	year = {2017},
	publisher = {AAAI Press},
	pages = {4278–4284},
	numpages = {7},
	location = {San Francisco, California, USA},
	series = {AAAI'17}
}


@INPROCEEDINGS{huang2017,
	author={G. {Huang} and Z. {Liu} and L. {Van Der Maaten} and K. Q. {Weinberger}},
	booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	title={Densely Connected Convolutional Networks},
	year={2017},
	volume={},
	number={},
	pages={2261-2269},
	doi={10.1109/CVPR.2017.243}}


 @InProceedings{pham2018,
 	title = {Efficient Neural Architecture Search via Parameters Sharing},
 	author = {Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
 	 booktitle = {Proceedings of the 35th International Conference on Machine Learning},
 	 pages = {4095--4104},
 	 year = {2018},
 	 editor = {Jennifer Dy and Andreas Krause},
 	 volume = {80},
 	 series = {Proceedings of Machine Learning Research},
 	 address = {Stockholmsmässan, Stockholm Sweden},
 	 month = {10--15 Jul},
 	  publisher = {PMLR}

 }

 @inproceedings{chollet2017,
 	author={F. Chollet},
 	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 	title={Xception: Deep Learning with Depthwise Separable Convolutions},
 	volume={},
 	number={},
 	pages={1800-1807},
 	doi={10.1109/CVPR.2017.195},
 	ISSN={1063-6919},
 	month={July},
 	year={2017}}


@misc{howard2017,
	added-at = {2020-05-11T23:24:49.000+0200},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	eprint={1704.04861},
	archivePrefix={arXiv},
	title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
	Applications},

	year = 2017
}


@INPROCEEDINGS{sandler2018,
	author={M. {Sandler} and A. {Howard} and M. {Zhu} and A. {Zhmoginov} and L. {Chen}},
	booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
	year={2018},
	volume={},
	number={},
	pages={4510-4520},
	doi={10.1109/CVPR.2018.00474}}



 @InProceedings{tan2019, title = {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks}, author = {Tan, Mingxing and Le, Quoc}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {6105--6114}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, address = {Long Beach, California, USA}, month = {09--15 Jun}, publisher = {PMLR} }

 @INPROCEEDINGS{szegedy2016,
 	author={C. {Szegedy} and V. {Vanhoucke} and S. {Ioffe} and J. {Shlens} and Z. {Wojna}},
 	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 	title={Rethinking the Inception Architecture for Computer Vision},
 	year={2016},
 	volume={},
 	number={},
 	pages={2818-2826},
 	doi={10.1109/CVPR.2016.308}}


 @article{ILSVRC15,
 	Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
 	Title = {{ImageNet Large Scale Visual Recognition Challenge}},
 	Year = {2015},
 	journal   = {International Journal of Computer Vision (IJCV)},
 	doi = {10.1007/s11263-015-0816-y},
 	volume={115},
 	number={3},
 	pages={211-252}
 }

@inproceedings{Kingma14,
	author = {Kingma, Diederik P. and Ba, Jimmy},
	booktitle = {3rd International Conference of Learning Representations (ICLR)},
	title = {Adam: A Method for Stochastic Optimization.},
	month = {December},
	year = {2014}
}

@ARTICLE{sanchez2020,
	author={R. {Sanchez-Iborra} and A. F. {Skarmeta}},
	journal={IEEE Circuits and Systems Magazine},
	title={TinyML-Enabled Frugal Smart Objects: Challenges and Opportunities},
	year={2020},
	volume={20},
	number={3},
	pages={4-18},
	doi={10.1109/MCAS.2020.3005467}}

@article{French99,
	author={French, Robert M.},
	doi={10.1016/S1364-6613(99)01294-2},
	isbn={13646613},
	issn={13646613},
	journal={Trends in Cognitive Sciences},
	number={4},
	pages={128--135},
	pmid={10322466},
	title={Catastrophic forgetting in connectionist networks},
	volume={3},
	year={1999},
	keywords={Classic},
}

@INPROCEEDINGS{zhou2020,
	author={Zhou, Yan and Chen, Shaochang and Wang, Yiming and Huan, Wenming},
	booktitle={2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)}, 
	title={Review of research on lightweight convolutional neural networks}, 
	year={2020},
	volume={},
	number={},
	pages={1713-1720},
	doi={10.1109/ITOEC49072.2020.9141847}}

@ARTICLE{jeon2021,
	author={Jeon, Young Seok and Yoshino, Kensuke and Hagiwara, Shigeo and Watanabe, Atsuya and Quek, Swee Tian and Yoshioka, Hiroshi and Feng, Mengling},
	journal={IEEE Journal of Biomedical and Health Informatics}, 
	title={Interpretable and Lightweight 3-D Deep Learning Model for Automated ACL Diagnosis}, 
	year={2021},
	volume={25},
	number={7},
	pages={2388-2397},
	doi={10.1109/JBHI.2021.3081355}}

@InProceedings{hui2018,
	author = {Hui, Tak-Wai and Tang, Xiaoou and Loy, Chen Change},
	title = {LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2018}
} 

@ARTICLE{qiang2021,
	author={Qiang, Baohua and Zhai, Yijie and Zhou, Mingliang and Yang, Xianyi and Peng, Bo and Wang, Yufeng and Pang, Yuanchao},
	journal={IEEE Access}, 
	title={SqueezeNet and Fusion Network-Based Accurate Fast Fully Convolutional Network for Hand Detection and Gesture Recognition}, 
	year={2021},
	volume={9},
	number={},
	pages={77661-77674},
	doi={10.1109/ACCESS.2021.3079337}}

@inproceedings{Yu2016
	author = {Fisher Yu and Vladlen Koltun},
	title = {Multi-Scale Context Aggregation by Dilated Convolutions},
	booktitle = {International Conference on Learning Representations (ICLR)},
	year = {2016},
	month = {may}
}


@inproceedings{Lin2017,
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross B. and He, Kaiming and Hariharan, Bharath and Belongie, Serge J.},
	booktitle = {CVPR},
	ee = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2017.106},
	keywords = {ba-2018-hahnrico},
	pages = {936-944},
	publisher = {IEEE Computer Society},
	title = {Feature Pyramid Networks for Object Detection.},
	year = 2017
}

@book{kuncheva2004,
	author = {Kuncheva, Ludmila I.},
	title = {Combining Pattern Classifiers: Methods and Algorithms},
	year = {2004},
	isbn = {0471210781},
	publisher = {Wiley-Interscience},
	address = {USA}
}

@article{evci2022,
	author    = {Utku Evci and
	Vincent Dumoulin and
	Hugo Larochelle and
	Michael C. Mozer},
	title     = {Head2Toe: Utilizing Intermediate Representations for Better Transfer
	Learning},
	journal   = {ArXiv Preprint},
	volume    = {abs/2201.03529},
	year      = {2022},
	url       = {https://arxiv.org/abs/2201.03529},
	eprinttype = {arXiv},
	eprint    = {2201.03529},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}